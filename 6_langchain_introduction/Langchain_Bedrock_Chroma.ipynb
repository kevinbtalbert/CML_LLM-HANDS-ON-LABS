{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3893c255-5fe4-4dd7-9d17-2273051163ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Advanced Q&A with AWS Bedrock with Langchain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809159d-b739-4df0-b9fd-1648e38edf4b",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter notebook guide, where we delve into: Langchain, Chroma DB, and AWS Bedrock. This notebook is designed to walk you through the setup and application of these tools in a question-answering context, leveraging the strengths of each to create a robust and intelligent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d12bc0-e516-4ba7-bb5d-493e40de6fbb",
   "metadata": {},
   "source": [
    "- **Configuring AWS Bedrock:** AWS Bedrock's large language models will serve as the backbone of our system, providing the computational power and linguistic understanding necessary for processing complex queries.\n",
    "\n",
    "- **Initializing Chroma DB Client:** Next, we'll establish a connection with Chroma DB, a vector database designed for efficient storage and retrieval of data. This step is crucial for managing the knowledge our model will access.\n",
    "\n",
    "- **Setting Up Langchain:** We'll start by setting up Langchain, an innovative toolkit that allows us to seamlessly blend the capabilities of large language models with external databases.\n",
    "\n",
    "- **Running a Practical Example:** Once our setup is complete, we'll run a demonstration function. This function showcases how to utilize Langchain, Bedrock, and Chroma DB in unison to answer questions. We'll use vector data stored in Chroma DB to feed our language model with relevant information, enhancing the accuracy and relevance of its responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de600824-8843-4d80-adf2-9373246ee4ab",
   "metadata": {},
   "source": [
    "## Configuring AWS Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bdb65-fcb0-4a8c-995f-1bee1f45cbca",
   "metadata": {},
   "source": [
    "Let's start by configuring our AWS credentials as a profile for Langchain.\n",
    "\n",
    "The function below will use pre-set environment variables to create a credentials file with a **default** profile for Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa759dd7-632d-4ebf-9215-df426ea094f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS credentials file created at: /home/cdsw/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_aws_credentials_file():\n",
    "    # Retrieve environment variables\n",
    "    aws_region = os.environ.get('AWS_DEFAULT_REGION', 'default_region')\n",
    "    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID', 'default_access_key')\n",
    "    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'default_secret_key')\n",
    "\n",
    "    # Define the folder and file paths\n",
    "    aws_folder_path = os.path.expanduser('~/.aws')\n",
    "    credentials_file_path = os.path.join(aws_folder_path, 'credentials')\n",
    "\n",
    "    # Create the .aws directory if it does not exist\n",
    "    if not os.path.exists(aws_folder_path):\n",
    "        os.makedirs(aws_folder_path)\n",
    "\n",
    "    # Write the credentials to the file\n",
    "    with open(credentials_file_path, 'w') as credentials_file:\n",
    "        credentials_file.write('[default]\\n')\n",
    "        credentials_file.write(f'aws_access_key_id={aws_access_key_id}\\n')\n",
    "        credentials_file.write(f'aws_secret_access_key={aws_secret_access_key}\\n')\n",
    "        credentials_file.write(f'region={aws_region}\\n')\n",
    "\n",
    "    print(f\"AWS credentials file created at: {credentials_file_path}\")\n",
    "    \n",
    "create_aws_credentials_file()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809e286-4346-4b11-bc41-5f31706dd09a",
   "metadata": {},
   "source": [
    "## Initializing Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79215a9-bbed-49c0-a223-8d962a3623ac",
   "metadata": {},
   "source": [
    "Next, we will configure the ChromaDB and use the same PersistentClient used in section 3c, please use the **3c_load_to_chroma_vector_db/populate_chroma_vectors.py** file to populate the vector DB with your data. \n",
    "\n",
    "We will use **cml-default** as the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd0b9c5-e997-41f8-b645-df7a0626b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"/home/cdsw/chroma-data\")\n",
    "COLLECTION_NAME = \"cml-default\"\n",
    "\n",
    "collection = persistent_client.get_or_create_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520a6f5-6bc8-4b9e-9acc-c28f947e0903",
   "metadata": {},
   "source": [
    "We will use [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model to embed our data in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c82968-9660-43b1-9b23-1d0e3a542926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "\n",
    "EMBEDDING_FUNCTION = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb377bfc-3628-4e06-8a96-6a4a6e384453",
   "metadata": {},
   "source": [
    "## Getting started with Langchain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d6995-8f19-45ed-b688-fcc509b10df3",
   "metadata": {},
   "source": [
    "Let's setup Langchain, we will start with configuring the Vector Store as Chroma using the persistent client we setup previously along with the embedding function and collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe72741-872b-4802-b164-adc8342ffe93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set up the Chroma vector store with the persistent client and collection name\n",
    "vectorstore = Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=EMBEDDING_FUNCTION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9bd8c-1050-47d2-8f76-1826804dcba2",
   "metadata": {},
   "source": [
    "AWS Bedrock can be configured with Langchain using an AWS credentials profile and the model name to be used. We will be using the **anthropic.claude-v2:1** model for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb234c5-b32e-4680-99bd-2089727e0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "LLM_MODEL = Bedrock(\n",
    "    credentials_profile_name=\"default\", model_id=\"anthropic.claude-v2:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39eb8e-0231-4c1f-b985-78d922066a9c",
   "metadata": {},
   "source": [
    "Let's add a simple prompt which instructs our LLM to use the retrieved context when answering the question. We will pass the Prompt Template to our Question and Answering Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217a1ec8-2ac0-4fce-9233-16f435b87b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Prompt Template for Langchain\n",
    "template = \"\"\"You are a helpful AI assistant. Use only the below provided Context to answer the following question. If you do not know the answer respond with \"I don't know.\"\n",
    "Context:{context}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91417687-658f-45ae-aa7a-fb91e82dd284",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we will pull everything together and create our Retrieval QA Chain using the Bedrock Model, Chroma Vectorstore and Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9dc3ea6-dbe0-4dec-ba93-023bc1e5254b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create the QA chain with the Chroma vector store\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=LLM_MODEL,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e42db9-c6d8-4c0c-864a-8f3348791f36",
   "metadata": {},
   "source": [
    "We can now call the QA Chain and ask questions to our documents. \n",
    "\n",
    "**Please set the QUESTION variable with your question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed1e289-ec8d-4d5b-932a-81d66b9bde38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:What does the Privacy Rule permit? \n",
      " \n",
      " Answer:The Privacy Rule permits, but does not require, a covered entity voluntarily to obtain patient consent for uses and disclosures of protected health information for treatment, payment, and health care operations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to call the QA chain on the provided question and return the answer as a result \n",
    "def generate_response(question,qa_chain):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "# Run the QA chain and Access the result\n",
    "QUESTION = \"What does the Privacy Rule permit?\"\n",
    "\n",
    "print(f\"Question:{QUESTION} \\n \\n Answer:{generate_response(QUESTION, qa_chain)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb0294-6b9a-405b-8418-a550397da78f",
   "metadata": {},
   "source": [
    "## Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8ac88-6229-4e2d-9541-d6875be60d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:What does the Privacy Rule permit? \n",
      " \n",
      " Answer:The Privacy Rule permits, but does not require, a covered entity voluntarily to obtain patient consent for uses and disclosures of protected health information for treatment, payment, and health care operations.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# Set up the persistent client\n",
    "persistent_client = chromadb.PersistentClient(path=\"/home/cdsw/chroma-data\")\n",
    "COLLECTION_NAME = \"cml-default\"\n",
    "# Create a collection in the persistent client\n",
    "collection = persistent_client.get_or_create_collection(COLLECTION_NAME)\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "EMBEDDING_FUNCTION = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Set up the Chroma vector store with the persistent client and collection name\n",
    "vectorstore = Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=EMBEDDING_FUNCTION\n",
    "    )\n",
    "\n",
    "\n",
    "LLM_MODEL = Bedrock(\n",
    "    credentials_profile_name=\"default\", model_id=\"anthropic.claude-v2:1\"\n",
    ")\n",
    "\n",
    "# Prompt Template for Langchain\n",
    "template = \"\"\"You are a helpful AI assistant. Use only the below provided Context to answer the following question. If you do not know the answer respond with \"I don't know.\"\n",
    "Context:{context}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# Create the QA chain with the Chroma vector store\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=LLM_MODEL,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "def generate_response(question,qa_chain):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "# Run the QA chain and Access the result\n",
    "QUESTION = \"What does the Privacy Rule permit?\"\n",
    "\n",
    "print(f\"Question:{QUESTION} \\n \\n Answer:{generate_response(QUESTION, qa_chain)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ca2f4-29e5-4bd3-9072-10db7e8e9774",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b33dc-d0bf-48d8-bfc4-3b4ebe4c1541",
   "metadata": {},
   "source": [
    "As we wrap up this Jupyter notebook, we've not only navigated through the process of setting up Langchain, Chroma DB, and AWS Bedrock but also successfully executed a question-answering function that leverages these technologies in unison. This has given us valuable insights into the integration of advanced language models with efficient data retrieval systems, showcasing the immense potential of AI in transforming how we interact with and process information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07b111-69be-4803-8af1-82b587154d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
